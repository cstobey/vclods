#PIPE Preform advanced ETL operations based on a temp table definition with structured comments. Should be followed by .(sql\|dst).batch.
ETL_TBL_TEMP_FILE="$(mktemp)"; add_file_cleanup "$ETL_TBL_TEMP_FILE"
TEMP_FILE="$(mktemp)"; add_file_cleanup "$TEMP_FILE"
: "${ETL_EXT_DIR:=$INPUT_DIR}" # Directory to look for .etl temp table definition files
: "${ETL_EXT_ERR_ON_EMPTY:=1}" # If non-zero, error when the input stream is empty.<br />If 1, also emit an error message.
: "${ETL_EXT_ROW_REP:=1}" # If 1, force ROW based replication.
: "${ext_opt:=${ETL_EXT_FILE:=$base_filename}}" # temp table definition filename.

sed 's/--.*$//' "$ETL_EXT_DIR/$ext_opt" >"$ETL_TBL_TEMP_FILE" # handle fifos; -- are real comments
tmp_tbl=$(grep -E '^CREATE (OR REPLACE )?(TEMPORARY )?TABLE' "$ETL_TBL_TEMP_FILE" | sed -r 's/^.* TABLE +([^ (]+)([( ].*)?$/\1/')
ingest_fields=$(grep -E '#(map|ingest|unique)' "$ETL_TBL_TEMP_FILE" | grep -E -v '#(key|ignore|include|append)' | awk '{print $1}' | paste -sd',')
all_directives='#(sync(_no_update)?|mode|l?join|where|append|include|ignore|ingest|key|unique|explode_[^ ]+|(map|generate|pivot)(_[^ ]+)?) '
working_fields=$(grep -E "$all_directives" "$ETL_TBL_TEMP_FILE" | sed -r 's/^ +//;s/ +/ /g;s/^([^# ]+ )[^#]*/\1/;s/(#[^ ]+ )/\1  /' | awk '/^[^#]/ {gsub(/ '"$all_directives"'/, "\n"$1"&", $0)} /^[#]/ {gsub(/'"$all_directives"'/, "\n&", $0)} 1' | grep "#")

cat >"$TEMP_FILE" # include the previous pipe's data... then test it
[ -s "$TEMP_FILE" ] || case "$ETL_EXT_ERR_ON_EMPTY" in 0) : ;; 1) echo >&2 "[WARNING] Empty input: $base_filename.etl-$ext_opt"; return 0 ;; *) return 0 ;; esac
awk 'NF {exit $1 !~ /^[(#]/}' || { echo >&2 "Invalid starting .etl line. Must be an INSERT row or a .batch directive"; return 1; }

[ "$ETL_EXT_ROW_REP" -eq 1 ] && echo "SET SESSION binlog_format='ROW';"
cat - "$TEMP_FILE" << EOF  # sed gets rid of directives to avoid confusing other potential stream readers
$(sed '/^ *#/ d;s/#.*$//' "$ETL_TBL_TEMP_FILE")
#RESET
#start INSERT INTO $tmp_tbl ($ingest_fields) VALUES
EOF
echo '#RESET'

function sync_tbl {
  tbl=$1
  shift
# TODO: generate a subselect the same as with the odku case so #generate_unique works... ie, use an actual subselect that is JOINed.
  grep -E "#([^ ]+\s+$tbl(\s|$)|explode)" <<<"$working_fields" | awk -v t="${tbl%%@*}" -v tt="$tmp_tbl" -f etl_main
}

grep '#explode_json_deep' -q <<<"$working_fields" && echo 'SET SESSION standard_compliant_cte = 0;'
grep '#explode' <<<"$working_fields" | awk -v tt="$tmp_tbl" -f etl_explode

grep -E -o '#(include|append|sync(_no_update)?) [^#]+' "$ETL_TBL_TEMP_FILE" | awk '
function cc_ws(value, sep, new) {return (value=="") ? new : value sep new}
function tail() {ret=""; for (i=2; i<=NF; i++) ret=cc_ws(ret," ",$i); return ret}
BEGIN {printf "cat <(echo)"} 
/^#include/ {printf "%s", " $ETL_EXT_DIR/"$2} 
/^#append/ {printf "%s", " <(echo \x22"tail()"\x22)";} 
/^#sync/ {printf "%s", " <(sync_tbl "$2")"}' | . /dev/stdin

#CMD Commands applied to a field in the Temp table create statement. These Commands (and the CREATE TEMPORARY TABLE statement they are attached to) must be in an extension options file. Stdin into the .etl extension must be a the VALUES part of the computed INSERT statement (the fields in the order of the CREATE TEMPORARY TABLE statement that exclusively `#ingest`, `#unique`, or `#map` commands attached to them). Fields may have multiple commands attached to them. `.etl` should always be followed by `.batch` unless you just want to test (ie, `do.sql.batch.etl-file.sh`)

# NOTE: "destination table" below refers to the table you are inserting into. To allow the same table to be referenced several different ways, the format is \<physical table name\>@\<identifier\> where only the physical table name is required.

## Stand-Alone Commands
# Order between #sync, #append, and #include commands indicates execution order. 
#TBL Command | Description
#sync | Command to sync the temp table with the destination table. First parameter is a destination table name, additional parameters explained below. When modified with `_no_update` (as #sync_no_update), any otherwise computed changes will not be UPDATEd.
#mode | Alter how a table is processed. Takes a destination table name and an option. See the availible options below.
#join | For the given table (first argument), JOIN in a given #explode sub-temp table (second argument)
#ljoin | #join, but do a LEFT JOIN instead of a JOIN
#where | Add a WHERE clause to most queries.
#include | Load a sql script file (with no .extension) to handle any additional reformatting or processing that is required. If added to a field line, also acts like #ignore.
#append | Add the rest of the line to the stream. Allows you to append manual queries without using #include. If added to a field line, also acts like #ignore.
#generate | Generate a virtual field that is not in the temp table. The SQL statements that follow the field name are used instead of a column name in the temp table when doing the ETL into the destination table. Used in place of either a VIRTUAL column on the temp table or an #include script to do the generation. See Modifiers below.
#generate_unique | A logical combination of #generate and #unique
#pivot | #generate but for #join fields that need to be pivoted on. After the field name, takes the #join table name, field, and expected value. 

## Commands after a field definition
# Unless specified, the following commands take the destination table and field name as parameters. Spaces are not allowed in table and field names. #generate follows the same format but is not attached to a field.
#TBL Command | Description
#ingest | Force this field to be ingested in the initial INSERT INTO tmp table. Takes no parameters.
#ignore | Do not ingest this field into the initial temp table. Takes no parameters.
#key | The auto_incrementing Primary key that will be used to sync deep FK chains. Will not be ingested, but rather derived after syncing with the destination table.
#unique | Unique fields candidate keys on the table. If there is no UNIQUE index, you can spoof the behavior with #unique_no_update. Does not need to be unique in the temp table (useful for deep FK chains). Multiple #unique fields for a single destination table act like a single index.
#map | A regular field on the given destination table. See Modifiers below.
#explode | Generate a sub-temp table that explodes a column (or virtual column like #generate). Attach this directive to an id column for the temp table. Use with #join.<br />There are several methods of exploding with different generated columns. See details below.

## #mode Options
# Specifies different methods of how a to sync a table. modes of the same Group exclude the use of other modes in that same Group.
#TBL Option | Group | Description
# odku | Layout | Uses INSERT INTO ... ON DUPLICATE KEY UPDATE to quickly sync the table. Can bloat the id space so best with log type tables.
# odku_ai_always | Layout | Same as odku but forces the AUTO_INCREMENTING keys not to bloat at the expence of an ALTER TABLE.
# odku_ai | Layout | The default: same as odku_always does its best to avoid needing to run the ALTER TABLE.
# ui_split | Layout | tries its best to not bloat AUTO_INCREMENTING keys by being more particular about managing whether it uses an UPDATE or INSERT. Best with lower cardinality tables.
# sparse | Quantity | Treats the temp table as sparse. All rows with NULL unique values are ignored. Only makes sense for leaf tables.
# not_exists | Presence_Test | The default: use a NOT EXISTS subselect to force absence when needed.
# not_in | Presence_Test | Use a ROW() NOT IN subselect to force absence when needed. Normally the slower option, but sometimes faster.

## #sync Optional Parameters
# These follow a destination table name
#TBL Option | Format | Description
# SET_NOT_PRESENT | \<SET clause without the SET\>\<Optional WHERE clause\> | UPDATEs any rows in the destination table that are not found in the temp table in the provided way.
# DELETE_NOT_PRESENT | \<Optional WHERE clause without the WHERE\> | DELETEs any rows in the destination table that are not found in the temp table and conform to the where clause.

## Modifiers for #mapi, #generate, and #pivot
# These are appened directly to the Command that they modify, so #map + _no_update would be #map_no_update.
#TBL Modifier | Description
# _no_update | Exclude this field from being updated if it otherwise would have been.
# _ifnull | The default: This field will be updated if the temp table value IS NOT NULL.
# _always | Update the field even if the temp table value IS NULL.
# _greatest | Update the field to the GREATEST value between the destination and temp tables. NULL safe.
# _least | Update the field to the LEAST value between the destination and temp tables. NULL safe.

## Modifiers for #explode
# All Sub tables have an `id` column that linkes to the main temp tables id field (that the #explode directive is attached to). The temp table id is what the #explode directive is attached to. The 2 id fields are used for normal #sync operations by using the #join directive.
#TBL Modifier | Columns | Description
# _csv | id, the_value, index_number | break up comma separated lists... quoting not supported (this is a super simple implementation)
# _json_shallow | id, the_value, the_key | From a JSON document, break up the top level Object OR Array. the_key is a string for Ojects or an int for Arrays.
# _json_deep | id, the_value, the_key, the_jpath | Recursively explode a JSON field, retaining all leaf nodes (including empty objects/arrays).
# _manual | User Defined (must include id) | takes an #include file instead of a #generate SQL clause so you can define your own temp table to use. Be sure to keep the temp table's name consistent.
